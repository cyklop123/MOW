{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Zbiór danych\n",
    "<p>\n",
    "Zbiór danych dotyczący zdatności wody do picia. Zawiera 20 cech, które przedstawiają zawartość poszczególnych związków chemicznych, pierwiastków i mikroorganizmów oraz cechę określającą zdatność do spożycia.\n",
    "</p>\n",
    "https://www.kaggle.com/mssmartypants/water-quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wczytanie zbioru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('datasets/waterQuality1.csv', delimiter=',')\n",
    "data = data[data.is_safe != '#NUM!']\n",
    "data = data[data.ammonia != '#NUM!']\n",
    "data['ammonia'] = pd.to_numeric(data['ammonia'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usuwanie losowych wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in data.iterrows():\n",
    "    if index % 10 == 0:\n",
    "        random_col = np.random.choice(data.columns)\n",
    "        data.at[index, random_col] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zastąpienie brakujących wartości średnią"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = data.mean(axis=0)\n",
    "for index, value in means.items():\n",
    "    data[index].fillna(value=value, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zastąpienie brakujących wartości medianą"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = data.median(axis=0)\n",
    "for index, value in medians.items():\n",
    "    data[index].fillna(value=value, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zastąpienie najczęściej występujacą wartością"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = data['is_safe'].value_counts().idxmax()\n",
    "data['is_safe'].fillna(value=value, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Statystyki opisowe i podsumowujące"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zależności między zmiennymi\n",
    "Scatter ploty między każdą parą zmiennych. Na przekątnej wykres gęstości prawdopodobieństwa (rozkład) zmiennej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zmiennej podejrzane o relacje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data[['bacteria', 'viruses']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabela korelacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(16, 9), dpi=80)\n",
    "\n",
    "corrMatrix = data.corr()\n",
    "sns.heatmap(corrMatrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogramy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)\n",
    "\n",
    "data.hist(figsize=(30, 30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmiennej podejrzane o relacje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data[['bacteria', 'viruses']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxploty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.boxplot(figsize=(30, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Odfiltrowanie wartości odstających"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "filtered = data[(np.abs(stats.zscore(data['aluminium'])) < 3)]\n",
    "filtered = filtered[(np.abs(stats.zscore(filtered['arsenic'])) < 3)]\n",
    "filtered = filtered[(np.abs(stats.zscore(filtered['nitrites'])) < 3)]\n",
    "\n",
    "filtered.boxplot(figsize=(30, 15))\n",
    "plt.show()\n",
    "filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Skalowanie cech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(data[['aluminium', 'ammonia', 'arsenic', 'barium', 'cadmium', 'chloramine', 'chromium', 'copper', 'flouride', 'bacteria', 'viruses', 'lead', 'nitrates', 'nitrites', 'mercury', 'perchlorate', 'radium', 'selenium', 'silver', 'uranium']])\n",
    "data_scaled = scaler.transform(filtered[['aluminium', 'ammonia', 'arsenic', 'barium', 'cadmium', 'chloramine', 'chromium', 'copper', 'flouride', 'bacteria', 'viruses', 'lead', 'nitrates', 'nitrites', 'mercury', 'perchlorate', 'radium', 'selenium', 'silver', 'uranium']])\n",
    "\n",
    "data_scaled = pd.DataFrame(np.append(data_scaled, filtered[['is_safe']].to_numpy(), axis=1), dtype=float)\n",
    "data_scaled.columns = data.columns\n",
    "\n",
    "data_scaled.boxplot(figsize=(30, 9))\n",
    "plt.show()\n",
    "data_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Redukcja wymiarowości\n",
    "### Podział train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_scaled.loc[:, data_scaled.columns[:-1]].values\n",
    "y = data_scaled.loc[:,['is_safe']].values.flatten()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "y_train.shape, y_test.shape\n",
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selekcja cech\n",
    "#### Sequenential Forward/Backward Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Możliwe metryki: (jeżeli nie podana, wybierana jest domyślna metryka z podanego kalsyfikatora, można też napisać własną funkcję score'ującą)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "list(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3) # przykładowy klasyfikator\n",
    "\n",
    "sfs = SequentialFeatureSelector(\n",
    "    knn,                           # klasyfikator\n",
    "    n_features_to_select=18,       # liczba oczekwianych cech\n",
    "    direction='forward',           # 'backward' || 'forward'\n",
    "    scoring=None)  # metoda obliczania jakości modelu\n",
    "\n",
    "sfs.fit(X_train, y_train)\n",
    "reduced = sfs.transform(X_train)\n",
    "reduced.shape, sfs.get_support()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ekstrakcja cech\n",
    "#### Algorytm PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=18)\n",
    "pca.fit(X_train)\n",
    "principalComponents = pca.transform(X_train)\n",
    "principalComponents.shape, pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Modele decyzyjne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generowanie próbek brakującej klasy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "oversample = SMOTE()\n",
    "X_train_OS, y_train_OS = oversample.fit_resample(X_train, y_train)\n",
    "principalComponents_OS = pca.transform(X_train_OS)\n",
    "reduced_OS = sfs.transform(X_train_OS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train_OS, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresja logistyczna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "#PCA\n",
    "lr = LogisticRegression(random_state=0).fit(principalComponents, y_train)\n",
    "X_test_transformed = pca.transform(X_test)\n",
    "y_pred = lr.predict(X_test_transformed)\n",
    "result = metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred)\n",
    "print('PCA: ', result)\n",
    "metrics.plot_confusion_matrix(lr, X_test_transformed, y_test)\n",
    "plt.show()\n",
    "\n",
    "#PCA + oversampling\n",
    "lr = LogisticRegression(random_state=0).fit(principalComponents_OS, y_train_OS)\n",
    "X_test_transformed = pca.transform(X_test)\n",
    "y_pred = lr.predict(X_test_transformed)\n",
    "result = metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred)\n",
    "print('PCA+oversampling: ', result)\n",
    "metrics.plot_confusion_matrix(lr, X_test_transformed, y_test)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#SFS\n",
    "lr = LogisticRegression(random_state=0).fit(reduced, y_train)\n",
    "X_test_reduced = sfs.transform(X_test)\n",
    "y_pred = lr.predict(X_test_reduced)\n",
    "result = metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred)\n",
    "print('SFS: ', result)\n",
    "metrics.plot_confusion_matrix(lr, X_test_reduced, y_test)\n",
    "plt.show()\n",
    "\n",
    "#SFS + oversampling\n",
    "lr = LogisticRegression(random_state=0).fit(reduced_OS, y_train_OS)\n",
    "X_test_reduced = sfs.transform(X_test)\n",
    "y_pred = lr.predict(X_test_reduced)\n",
    "result = metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred)\n",
    "print('SFS+oversampling: ', result)\n",
    "metrics.plot_confusion_matrix(lr, X_test_reduced, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algroytm k-najbliższych sąsiadów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#PCA\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(principalComponents, y_train)\n",
    "X_test_transformed = pca.transform(X_test)\n",
    "y_pred = neigh.predict(X_test_transformed)\n",
    "\n",
    "print(\"PCA:\", metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred) )\n",
    "metrics.plot_confusion_matrix(neigh, X_test_transformed, y_test)\n",
    "plt.show()\n",
    "\n",
    "#PCA + oversampling\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(principalComponents_OS, y_train_OS)\n",
    "X_test_transformed = pca.transform(X_test)\n",
    "y_pred = neigh.predict(X_test_transformed)\n",
    "\n",
    "print(\"PCA+oversampling:\", metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred) )\n",
    "metrics.plot_confusion_matrix(neigh, X_test_transformed, y_test)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#SFS\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(reduced, y_train)\n",
    "X_test_reduced = sfs.transform(X_test)\n",
    "y_pred = neigh.predict(X_test_reduced)\n",
    "\n",
    "print(\"SFS:\", metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred) )\n",
    "metrics.plot_confusion_matrix(neigh, X_test_reduced, y_test)\n",
    "plt.show()\n",
    "\n",
    "#SFS + oversampling\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(reduced_OS, y_train_OS)\n",
    "X_test_reduced = sfs.transform(X_test)\n",
    "y_pred = neigh.predict(X_test_reduced)\n",
    "\n",
    "print(\"SFS + oversampling:\", metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred) )\n",
    "metrics.plot_confusion_matrix(neigh, X_test_reduced, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maszyna wektorów nośnych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#PCA\n",
    "svc = LinearSVC(max_iter=5000)\n",
    "svc.fit(principalComponents, y_train)\n",
    "X_test_transformed = pca.transform(X_test)\n",
    "y_pred = svc.predict(X_test_transformed)\n",
    "\n",
    "print(\"PCA:\", metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred) )\n",
    "metrics.plot_confusion_matrix(svc, X_test_transformed, y_test)\n",
    "plt.show()\n",
    "\n",
    "#PCA + balanced\n",
    "svc = LinearSVC(class_weight='balanced', max_iter=5000)\n",
    "svc.fit(principalComponents, y_train)\n",
    "X_test_transformed = pca.transform(X_test)\n",
    "y_pred = svc.predict(X_test_transformed)\n",
    "\n",
    "print(\"PCA + balanced:\", metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred) )\n",
    "metrics.plot_confusion_matrix(svc, X_test_transformed, y_test)\n",
    "plt.show()\n",
    "\n",
    "#PCA + oversampling\n",
    "svc = LinearSVC(max_iter=5000)\n",
    "svc.fit(principalComponents_OS, y_train_OS)\n",
    "X_test_transformed = pca.transform(X_test)\n",
    "y_pred = svc.predict(X_test_transformed)\n",
    "\n",
    "print(\"PCA + oversampling:\", metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred) )\n",
    "metrics.plot_confusion_matrix(svc, X_test_transformed, y_test)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#SFS\n",
    "svc =  LinearSVC(max_iter=5000)\n",
    "svc.fit(reduced, y_train)\n",
    "X_test_reduced = sfs.transform(X_test)\n",
    "y_pred = svc.predict(X_test_reduced)\n",
    "\n",
    "print(\"SFS:\", metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred) )\n",
    "metrics.plot_confusion_matrix(svc, X_test_reduced, y_test)\n",
    "plt.show()\n",
    "\n",
    "#SFS + balanced\n",
    "svc =  LinearSVC(class_weight='balanced', max_iter=5000)\n",
    "svc.fit(reduced, y_train)\n",
    "X_test_reduced = sfs.transform(X_test)\n",
    "y_pred = svc.predict(X_test_reduced)\n",
    "\n",
    "print(\"SFS + balanced:\", metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred) )\n",
    "metrics.plot_confusion_matrix(svc, X_test_reduced, y_test)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#SFS + oversampling\n",
    "svc =  LinearSVC(class_weight='balanced', max_iter=5000)\n",
    "svc.fit(reduced_OS, y_train_OS)\n",
    "X_test_reduced = sfs.transform(X_test)\n",
    "y_pred = svc.predict(X_test_reduced)\n",
    "\n",
    "print(\"SFS + oversampling:\", metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred) )\n",
    "metrics.plot_confusion_matrix(svc, X_test_reduced, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drzewo decyzyjne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "#PCA\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.fit(principalComponents, y_train)\n",
    "X_test_transformed = pca.transform(X_test)\n",
    "y_pred = dt.predict(X_test_transformed)\n",
    "\n",
    "print(\"PCA:\", metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred) )\n",
    "metrics.plot_confusion_matrix(dt, X_test_transformed, y_test)\n",
    "plt.show()\n",
    "\n",
    "#PCA + oversampling\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.fit(principalComponents, y_train)\n",
    "X_test_transformed = pca.transform(X_test)\n",
    "y_pred = dt.predict(X_test_transformed)\n",
    "\n",
    "print(\"PCA + oversampling:\", metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred) )\n",
    "metrics.plot_confusion_matrix(dt, X_test_transformed, y_test)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#SFS\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.fit(reduced, y_train)\n",
    "X_test_reduced = sfs.transform(X_test)\n",
    "y_pred = dt.predict(X_test_reduced)\n",
    "\n",
    "print(\"SFS:\", metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred) )\n",
    "metrics.plot_confusion_matrix(dt, X_test_reduced, y_test)\n",
    "plt.show()\n",
    "\n",
    "#SFS + oversampling\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.fit(reduced, y_train)\n",
    "X_test_reduced = sfs.transform(X_test)\n",
    "y_pred = dt.predict(X_test_reduced)\n",
    "\n",
    "print(\"SFS + oversampling:\", metrics.accuracy_score(y_test, y_pred), metrics.f1_score(y_test, y_pred) )\n",
    "metrics.plot_confusion_matrix(dt, X_test_reduced, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podsumowanie\n",
    "Najlepszy model pod względem accuracy score i F1 score: Decission tree + SFS + oversampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
